{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n",
      "/anaconda3/lib/python3.7/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "import re\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.model_selection import train_test_split\n",
    "twitter = Twitter()\n",
    "from collections import namedtuple\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
    "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "#import Early Stopping\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>dict</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>새끼</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>남자</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>진짜</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>여자</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사람</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  frequency  dict  class\n",
       "0   새끼         64     1      1\n",
       "1   남자         37     1      0\n",
       "2   진짜         36     1      0\n",
       "3   여자         32     1      0\n",
       "4   사람         27     1      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/SAEROM/slang_dict_twitter(1).csv\", encoding='cp949')\n",
    "#첫 5행을 불러옴\n",
    "dataset.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 160 200\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.2, shuffle = True)\n",
    "train, valid = train_test_split(train, test_size=0.2, shuffle = True)\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        #self.data=[]\n",
    "        #self.label=[]\n",
    "        \n",
    "        #word,freq,dict,tag라는 라벨을 갖고 있는 namedtuple 클래스를 생성.\n",
    "        TaggedWord = namedtuple('TaggedWord', 'word freq dict tag')\n",
    "        tagged_words = [TaggedWord(w, f, d, c) for w,f,d,c \n",
    "                              in data[[\"word\", \"frequency\", \"dict\", \"class\"]].values]\n",
    "        #print(tagged_words)\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        \n",
    "        \n",
    "        for t in tagged_words:\n",
    "            vector_=[]\n",
    "            vector_.append(t.freq)\n",
    "            vector_.append(t.dict)\n",
    "            \n",
    "        \n",
    "            data_x.append(vector_)\n",
    "            data_y.append(t.tag)\n",
    "        #print(data_x)\n",
    "        #print(data_y)\n",
    "\n",
    "        #print(type(np.array(data_x)))\n",
    "        #np.array(data_y)\n",
    "        \n",
    "        self.data = np.array(data_x)\n",
    "        self.label = np.array(data_y)\n",
    "        \n",
    "        print(\"<데이터의 shape>\")\n",
    "        print(type(self.data))\n",
    "        print(np.array(self.data).shape)\n",
    "        print(\"<데이터 내용>\") \n",
    "        print(torch.tensor(self.data[0], dtype=torch.float))\n",
    "        print(\"<레이블 표시>\") \n",
    "        print(self.label[0])\n",
    "        print(\"<총 레이블 길이>\") \n",
    "        print(len(self.label)) \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index], dtype=torch.float), self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=2, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DNN (nn.Module):\n",
    "    def __init__(self):\n",
    "        super (DNN, self).__init__ ()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear (2,32),\n",
    "            nn.ELU())\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear (32, 16),\n",
    "            nn.ELU())\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear (16, 2),\n",
    "            nn.ELU())\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view (-1, 2)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "model = DNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model (model, batch_size, patience, epochs): \n",
    "    \n",
    "    ##################################################\n",
    "    #train and validate model#\n",
    "    ##################################################\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "  \n",
    "\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################  \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "        \n",
    "    for epoch in range(1, epochs+1):\n",
    "        for batch, (data, label) in enumerate (loader_train, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "    ###################\n",
    "    # validate the model #\n",
    "    ###################  \n",
    "        model.eval()\n",
    "        for data, label in loader_valid:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, label)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "    \n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_loss, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "            \n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<데이터의 shape>\n",
      "<class 'numpy.ndarray'>\n",
      "(640, 2)\n",
      "<데이터 내용>\n",
      "tensor([5., 1.])\n",
      "<레이블 표시>\n",
      "0\n",
      "<총 레이블 길이>\n",
      "640\n",
      "<데이터의 shape>\n",
      "<class 'numpy.ndarray'>\n",
      "(200, 2)\n",
      "<데이터 내용>\n",
      "tensor([1., 1.])\n",
      "<레이블 표시>\n",
      "0\n",
      "<총 레이블 길이>\n",
      "200\n",
      "<데이터의 shape>\n",
      "<class 'numpy.ndarray'>\n",
      "(160, 2)\n",
      "<데이터 내용>\n",
      "tensor([17.,  0.])\n",
      "<레이블 표시>\n",
      "1\n",
      "<총 레이블 길이>\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "data_train = Data (data = train)\n",
    "loader_train = torch.utils.data.DataLoader(data_train, batch_size = 20, shuffle = False)\n",
    "data_test = Data (data = test)\n",
    "loader_test = torch.utils.data.DataLoader(data_test, batch_size = 20, shuffle = False)\n",
    "data_valid = Data (data = valid)\n",
    "loader_valid = torch.utils.data.DataLoader(data_valid, batch_size = 20, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.57664 valid_loss: 0.53274\n",
      "Validation loss decreased (inf --> 0.532735).  Saving model ...\n",
      "[  2/100] train_loss: 0.52579 valid_loss: 0.51844\n",
      "Validation loss decreased (0.532735 --> 0.518436).  Saving model ...\n",
      "[  3/100] train_loss: 0.51576 valid_loss: 0.51022\n",
      "Validation loss decreased (0.518436 --> 0.510217).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4/100] train_loss: 0.50474 valid_loss: 0.49244\n",
      "Validation loss decreased (0.510217 --> 0.492444).  Saving model ...\n",
      "[  5/100] train_loss: 0.47556 valid_loss: 0.45592\n",
      "Validation loss decreased (0.492444 --> 0.455920).  Saving model ...\n",
      "[  6/100] train_loss: 0.44372 valid_loss: 0.43418\n",
      "Validation loss decreased (0.455920 --> 0.434185).  Saving model ...\n",
      "[  7/100] train_loss: 0.42825 valid_loss: 0.42528\n",
      "Validation loss decreased (0.434185 --> 0.425278).  Saving model ...\n",
      "[  8/100] train_loss: 0.42169 valid_loss: 0.42113\n",
      "Validation loss decreased (0.425278 --> 0.421130).  Saving model ...\n",
      "[  9/100] train_loss: 0.41834 valid_loss: 0.41892\n",
      "Validation loss decreased (0.421130 --> 0.418924).  Saving model ...\n",
      "[ 10/100] train_loss: 0.41643 valid_loss: 0.41757\n",
      "Validation loss decreased (0.418924 --> 0.417573).  Saving model ...\n",
      "[ 11/100] train_loss: 0.41527 valid_loss: 0.41668\n",
      "Validation loss decreased (0.417573 --> 0.416682).  Saving model ...\n",
      "[ 12/100] train_loss: 0.41450 valid_loss: 0.41605\n",
      "Validation loss decreased (0.416682 --> 0.416053).  Saving model ...\n",
      "[ 13/100] train_loss: 0.41396 valid_loss: 0.41559\n",
      "Validation loss decreased (0.416053 --> 0.415587).  Saving model ...\n",
      "[ 14/100] train_loss: 0.41357 valid_loss: 0.41523\n",
      "Validation loss decreased (0.415587 --> 0.415230).  Saving model ...\n",
      "[ 15/100] train_loss: 0.41327 valid_loss: 0.41495\n",
      "Validation loss decreased (0.415230 --> 0.414950).  Saving model ...\n",
      "[ 16/100] train_loss: 0.41304 valid_loss: 0.41472\n",
      "Validation loss decreased (0.414950 --> 0.414725).  Saving model ...\n",
      "[ 17/100] train_loss: 0.41286 valid_loss: 0.41454\n",
      "Validation loss decreased (0.414725 --> 0.414541).  Saving model ...\n",
      "[ 18/100] train_loss: 0.41272 valid_loss: 0.41439\n",
      "Validation loss decreased (0.414541 --> 0.414391).  Saving model ...\n",
      "[ 19/100] train_loss: 0.41260 valid_loss: 0.41427\n",
      "Validation loss decreased (0.414391 --> 0.414266).  Saving model ...\n",
      "[ 20/100] train_loss: 0.41250 valid_loss: 0.41416\n",
      "Validation loss decreased (0.414266 --> 0.414161).  Saving model ...\n",
      "[ 21/100] train_loss: 0.41241 valid_loss: 0.41407\n",
      "Validation loss decreased (0.414161 --> 0.414072).  Saving model ...\n",
      "[ 22/100] train_loss: 0.41234 valid_loss: 0.41400\n",
      "Validation loss decreased (0.414072 --> 0.413996).  Saving model ...\n",
      "[ 23/100] train_loss: 0.41228 valid_loss: 0.41393\n",
      "Validation loss decreased (0.413996 --> 0.413930).  Saving model ...\n",
      "[ 24/100] train_loss: 0.41223 valid_loss: 0.41387\n",
      "Validation loss decreased (0.413930 --> 0.413873).  Saving model ...\n",
      "[ 25/100] train_loss: 0.41218 valid_loss: 0.41382\n",
      "Validation loss decreased (0.413873 --> 0.413822).  Saving model ...\n",
      "[ 26/100] train_loss: 0.41214 valid_loss: 0.41378\n",
      "Validation loss decreased (0.413822 --> 0.413778).  Saving model ...\n",
      "[ 27/100] train_loss: 0.41211 valid_loss: 0.41374\n",
      "Validation loss decreased (0.413778 --> 0.413739).  Saving model ...\n",
      "[ 28/100] train_loss: 0.41208 valid_loss: 0.41370\n",
      "Validation loss decreased (0.413739 --> 0.413704).  Saving model ...\n",
      "[ 29/100] train_loss: 0.41205 valid_loss: 0.41367\n",
      "Validation loss decreased (0.413704 --> 0.413672).  Saving model ...\n",
      "[ 30/100] train_loss: 0.41202 valid_loss: 0.41364\n",
      "Validation loss decreased (0.413672 --> 0.413644).  Saving model ...\n",
      "[ 31/100] train_loss: 0.41200 valid_loss: 0.41362\n",
      "Validation loss decreased (0.413644 --> 0.413619).  Saving model ...\n",
      "[ 32/100] train_loss: 0.41198 valid_loss: 0.41360\n",
      "Validation loss decreased (0.413619 --> 0.413596).  Saving model ...\n",
      "[ 33/100] train_loss: 0.41196 valid_loss: 0.41357\n",
      "Validation loss decreased (0.413596 --> 0.413575).  Saving model ...\n",
      "[ 34/100] train_loss: 0.41195 valid_loss: 0.41356\n",
      "Validation loss decreased (0.413575 --> 0.413555).  Saving model ...\n",
      "[ 35/100] train_loss: 0.41193 valid_loss: 0.41354\n",
      "Validation loss decreased (0.413555 --> 0.413538).  Saving model ...\n",
      "[ 36/100] train_loss: 0.41192 valid_loss: 0.41352\n",
      "Validation loss decreased (0.413538 --> 0.413522).  Saving model ...\n",
      "[ 37/100] train_loss: 0.41190 valid_loss: 0.41351\n",
      "Validation loss decreased (0.413522 --> 0.413507).  Saving model ...\n",
      "[ 38/100] train_loss: 0.41189 valid_loss: 0.41349\n",
      "Validation loss decreased (0.413507 --> 0.413494).  Saving model ...\n",
      "[ 39/100] train_loss: 0.41188 valid_loss: 0.41348\n",
      "Validation loss decreased (0.413494 --> 0.413481).  Saving model ...\n",
      "[ 40/100] train_loss: 0.41187 valid_loss: 0.41347\n",
      "Validation loss decreased (0.413481 --> 0.413470).  Saving model ...\n",
      "[ 41/100] train_loss: 0.41186 valid_loss: 0.41346\n",
      "Validation loss decreased (0.413470 --> 0.413459).  Saving model ...\n",
      "[ 42/100] train_loss: 0.41185 valid_loss: 0.41345\n",
      "Validation loss decreased (0.413459 --> 0.413449).  Saving model ...\n",
      "[ 43/100] train_loss: 0.41185 valid_loss: 0.41344\n",
      "Validation loss decreased (0.413449 --> 0.413440).  Saving model ...\n",
      "[ 44/100] train_loss: 0.41184 valid_loss: 0.41343\n",
      "Validation loss decreased (0.413440 --> 0.413431).  Saving model ...\n",
      "[ 45/100] train_loss: 0.41183 valid_loss: 0.41342\n",
      "Validation loss decreased (0.413431 --> 0.413423).  Saving model ...\n",
      "[ 46/100] train_loss: 0.41183 valid_loss: 0.41342\n",
      "Validation loss decreased (0.413423 --> 0.413415).  Saving model ...\n",
      "[ 47/100] train_loss: 0.41182 valid_loss: 0.41341\n",
      "Validation loss decreased (0.413415 --> 0.413408).  Saving model ...\n",
      "[ 48/100] train_loss: 0.41181 valid_loss: 0.41340\n",
      "Validation loss decreased (0.413408 --> 0.413402).  Saving model ...\n",
      "[ 49/100] train_loss: 0.41181 valid_loss: 0.41340\n",
      "Validation loss decreased (0.413402 --> 0.413396).  Saving model ...\n",
      "[ 50/100] train_loss: 0.41180 valid_loss: 0.41339\n",
      "Validation loss decreased (0.413396 --> 0.413390).  Saving model ...\n",
      "[ 51/100] train_loss: 0.41180 valid_loss: 0.41338\n",
      "Validation loss decreased (0.413390 --> 0.413384).  Saving model ...\n",
      "[ 52/100] train_loss: 0.41179 valid_loss: 0.41338\n",
      "Validation loss decreased (0.413384 --> 0.413379).  Saving model ...\n",
      "[ 53/100] train_loss: 0.41179 valid_loss: 0.41337\n",
      "Validation loss decreased (0.413379 --> 0.413374).  Saving model ...\n",
      "[ 54/100] train_loss: 0.41179 valid_loss: 0.41337\n",
      "Validation loss decreased (0.413374 --> 0.413370).  Saving model ...\n",
      "[ 55/100] train_loss: 0.41178 valid_loss: 0.41337\n",
      "Validation loss decreased (0.413370 --> 0.413365).  Saving model ...\n",
      "[ 56/100] train_loss: 0.41178 valid_loss: 0.41336\n",
      "Validation loss decreased (0.413365 --> 0.413361).  Saving model ...\n",
      "[ 57/100] train_loss: 0.41178 valid_loss: 0.41336\n",
      "Validation loss decreased (0.413361 --> 0.413357).  Saving model ...\n",
      "[ 58/100] train_loss: 0.41177 valid_loss: 0.41335\n",
      "Validation loss decreased (0.413357 --> 0.413354).  Saving model ...\n",
      "[ 59/100] train_loss: 0.41177 valid_loss: 0.41335\n",
      "Validation loss decreased (0.413354 --> 0.413350).  Saving model ...\n",
      "[ 60/100] train_loss: 0.41177 valid_loss: 0.41335\n",
      "Validation loss decreased (0.413350 --> 0.413347).  Saving model ...\n",
      "[ 61/100] train_loss: 0.41177 valid_loss: 0.41334\n",
      "Validation loss decreased (0.413347 --> 0.413344).  Saving model ...\n",
      "[ 62/100] train_loss: 0.41176 valid_loss: 0.41334\n",
      "Validation loss decreased (0.413344 --> 0.413341).  Saving model ...\n",
      "[ 63/100] train_loss: 0.41176 valid_loss: 0.41334\n",
      "Validation loss decreased (0.413341 --> 0.413338).  Saving model ...\n",
      "[ 64/100] train_loss: 0.41176 valid_loss: 0.41334\n",
      "Validation loss decreased (0.413338 --> 0.413335).  Saving model ...\n",
      "[ 65/100] train_loss: 0.41176 valid_loss: 0.41333\n",
      "Validation loss decreased (0.413335 --> 0.413333).  Saving model ...\n",
      "[ 66/100] train_loss: 0.41175 valid_loss: 0.41333\n",
      "Validation loss decreased (0.413333 --> 0.413330).  Saving model ...\n",
      "[ 67/100] train_loss: 0.41175 valid_loss: 0.41333\n",
      "Validation loss decreased (0.413330 --> 0.413328).  Saving model ...\n",
      "[ 68/100] train_loss: 0.41175 valid_loss: 0.41333\n",
      "Validation loss decreased (0.413328 --> 0.413326).  Saving model ...\n",
      "[ 69/100] train_loss: 0.41175 valid_loss: 0.41332\n",
      "Validation loss decreased (0.413326 --> 0.413324).  Saving model ...\n",
      "[ 70/100] train_loss: 0.41175 valid_loss: 0.41332\n",
      "Validation loss decreased (0.413324 --> 0.413322).  Saving model ...\n",
      "[ 71/100] train_loss: 0.41175 valid_loss: 0.41332\n",
      "Validation loss decreased (0.413322 --> 0.413320).  Saving model ...\n",
      "[ 72/100] train_loss: 0.41174 valid_loss: 0.41332\n",
      "Validation loss decreased (0.413320 --> 0.413318).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73/100] train_loss: 0.41174 valid_loss: 0.41332\n",
      "Validation loss decreased (0.413318 --> 0.413316).  Saving model ...\n",
      "[ 74/100] train_loss: 0.41174 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413316 --> 0.413314).  Saving model ...\n",
      "[ 75/100] train_loss: 0.41174 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413314 --> 0.413312).  Saving model ...\n",
      "[ 76/100] train_loss: 0.41174 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413312 --> 0.413311).  Saving model ...\n",
      "[ 77/100] train_loss: 0.41174 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413311 --> 0.413309).  Saving model ...\n",
      "[ 78/100] train_loss: 0.41174 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413309 --> 0.413308).  Saving model ...\n",
      "[ 79/100] train_loss: 0.41173 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413308 --> 0.413307).  Saving model ...\n",
      "[ 80/100] train_loss: 0.41173 valid_loss: 0.41331\n",
      "Validation loss decreased (0.413307 --> 0.413305).  Saving model ...\n",
      "[ 81/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413305 --> 0.413304).  Saving model ...\n",
      "[ 82/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413304 --> 0.413303).  Saving model ...\n",
      "[ 83/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413303 --> 0.413301).  Saving model ...\n",
      "[ 84/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413301 --> 0.413300).  Saving model ...\n",
      "[ 85/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413300 --> 0.413299).  Saving model ...\n",
      "[ 86/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413299 --> 0.413298).  Saving model ...\n",
      "[ 87/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413298 --> 0.413297).  Saving model ...\n",
      "[ 88/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413297 --> 0.413296).  Saving model ...\n",
      "[ 89/100] train_loss: 0.41173 valid_loss: 0.41330\n",
      "Validation loss decreased (0.413296 --> 0.413295).  Saving model ...\n",
      "[ 90/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413295 --> 0.413294).  Saving model ...\n",
      "[ 91/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413294 --> 0.413293).  Saving model ...\n",
      "[ 92/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413293 --> 0.413292).  Saving model ...\n",
      "[ 93/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413292 --> 0.413292).  Saving model ...\n",
      "[ 94/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413292 --> 0.413291).  Saving model ...\n",
      "[ 95/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413291 --> 0.413290).  Saving model ...\n",
      "[ 96/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413290 --> 0.413289).  Saving model ...\n",
      "[ 97/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413289 --> 0.413289).  Saving model ...\n",
      "[ 98/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413289 --> 0.413288).  Saving model ...\n",
      "[ 99/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413288 --> 0.413287).  Saving model ...\n",
      "[100/100] train_loss: 0.41172 valid_loss: 0.41329\n",
      "Validation loss decreased (0.413287 --> 0.413286).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# early stopping patience; how long to wait after last time validation loss improved.\n",
    "#patience = 20\n",
    "model, train_loss, valid_loss = train_model(model, \n",
    "                                            batch_size=20,epochs=100 , patience = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, batch_size):\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "\n",
    "    model.eval() # prep model for evaluation\n",
    "\n",
    "    for data, target in loader_test:\n",
    "        if len(target.data) != batch_size:\n",
    "            break\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item()*data.size(0)\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        print(target)\n",
    "        print(data)\n",
    "        print(pred)\n",
    "        for i in range(batch_size):\n",
    "            label = target.data[i]\n",
    "            #print('#############')\n",
    "            #print(, end=' ')\n",
    "            #print(target.data.view_as)\n",
    "            #print(loader_test.dataset)\n",
    "            #print(\"\\t\", label, \"\\t\" , target.data.view_as(pred))\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "        # calculate and print avg test loss\n",
    "        test_loss = test_loss/len(test)\n",
    "        print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                str(i), 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        #else:\n",
    "        #    print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #return target.data, label, correct\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([[1., 1.],\n",
      "        [2., 1.],\n",
      "        [4., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [4., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])\n",
      "Test Loss: 0.041329\n",
      "\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[ 2.,  1.],\n",
      "        [ 7.,  0.],\n",
      "        [ 2.,  1.],\n",
      "        [11.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 6.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [32.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 3.,  1.]])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "Test Loss: 0.036532\n",
      "\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 4.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 3.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 3.,  1.],\n",
      "        [15.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 3.,  1.],\n",
      "        [ 3.,  1.],\n",
      "        [14.,  0.]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1])\n",
      "Test Loss: 0.051512\n",
      "\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[ 1.,  0.],\n",
      "        [ 7.,  1.],\n",
      "        [ 4.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [20.,  1.],\n",
      "        [24.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [12.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "Test Loss: 0.036587\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [16.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 5.,  0.],\n",
      "        [ 2.,  0.],\n",
      "        [ 5.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.]])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0])\n",
      "Test Loss: 0.031515\n",
      "\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0])\n",
      "tensor([[ 2.,  1.],\n",
      "        [ 4.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 3.,  1.],\n",
      "        [12.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [26.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 3.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 2.,  1.],\n",
      "        [ 5.,  1.],\n",
      "        [ 2.,  1.]])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "Test Loss: 0.046486\n",
      "\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 4.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 9.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [10.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.]])\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "Test Loss: 0.046560\n",
      "\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 4.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [10.,  1.],\n",
      "        [ 5.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [ 7.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  0.]])\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "Test Loss: 0.031564\n",
      "\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([[ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 2.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 2.,  1.],\n",
      "        [ 2.,  0.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [ 1.,  1.],\n",
      "        [16.,  1.],\n",
      "        [ 2.,  1.]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "Test Loss: 0.041486\n",
      "\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [3., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.],\n",
      "        [4., 1.],\n",
      "        [2., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [4., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [3., 0.],\n",
      "        [1., 1.],\n",
      "        [2., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "Test Loss: 0.036537\n",
      "\n",
      "Test Accuracy of     0: 98% (153/155)\n",
      "Test Accuracy of     1: 66% (30/45)\n",
      "\n",
      "Test Accuracy (Overall): 91% (183/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "test_model(model, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-069857a2ff1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 in test[[\"word\", \"frequency\", \"dict\", \"class\"]].values]\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"단어 : \"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\t실제 레이블 : \"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"\\t예측 레이블 : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "TaggedWord = namedtuple('TaggedWord', 'word freq dict tag')\n",
    "tagged_words = [TaggedWord(w, f, d, c) for w,f,d,c \n",
    "                in test[[\"word\", \"frequency\", \"dict\", \"class\"]].values]\n",
    "for test in tagged_words:\n",
    "    if test.word in w2v_model: \n",
    "        print( \"단어 : \" ,test.word, \"\\t실제 레이블 : \" ,test.tag ,\"\\t예측 레이블 : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
